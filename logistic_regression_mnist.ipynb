{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Logistic Regression with a Neural Network mindset\n",
    "\n",
    "After completed the first course of Deep Learning Specialization on Coursera taught by Andrew Ng, I am implementing the algorithms on my own to retain the material taught.\n",
    "\n",
    "In this first notebook, the logistic regression algorithm is applied in the MNIST dataset which can be found at http://yann.lecun.com/exdb/mnist/ or at https://www.kaggle.com/c/digit-recognizer/data. \n",
    "\n",
    "Since it is a multclass dataset I have had to do some research since multiclass problem wasn't covered in the first course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from load_data import loadData\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 4, 4, 6, 9, 1, 5, 9, 6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainImages, trainLabels =  loadData(\"data/train-images-idx3-ubyte\", \"data/train-labels-idx1-ubyte\")\n",
    "testImages, testLabels =  loadData(\"data/t10k-images-idx3-ubyte\", \"data/t10k-labels-idx1-ubyte\")\n",
    "\n",
    "# Sanity check\n",
    "assert trainImages.shape == (60000, 28, 28)\n",
    "assert trainLabels.shape == (60000, )\n",
    "\n",
    "assert testImages.shape == (10000, 28, 28)\n",
    "assert testLabels.shape == (10000, )\n",
    "\n",
    "labels = []\n",
    "# plot 9 a random image with matplotlib and its labels for sanity check\n",
    "for i in range(1, 10):\n",
    "    index = np.random.choice(59999)\n",
    "    labels.append(trainLabels[index])\n",
    "    plt.subplot(330 + i)\n",
    "    plt.imshow(trainImages[index], cmap='Greys')\n",
    "print(labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trainning examples m_train:  60000\n",
      "Number of test examples m_train:  10000\n",
      "Height and Widht of each image:  28\n",
      "X - trainImages shape:  (60000, 28, 28)\n",
      "Y - trainLabels shape:  (60000,)\n",
      "X - testImages shape:  (10000, 28, 28)\n",
      "Y - testLabels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Get familiarized with the number of samples and shapes of the matrices \n",
    "m_train = len(trainImages)\n",
    "m_test = len(testImages)\n",
    "num_px = np.shape(trainImages[0])[0]\n",
    "\n",
    "print(\"Number of Trainning examples m_train: \", m_train)\n",
    "print(\"Number of test examples m_train: \", m_test)\n",
    "print(\"Height and Widht of each image: \", num_px)\n",
    "print(\"X - trainImages shape: \", np.shape(trainImages))\n",
    "print(\"Y - trainLabels shape: \", np.shape(trainLabels))\n",
    "print(\"X - testImages shape: \", np.shape(testImages))\n",
    "print(\"Y - testLabels shape: \", np.shape(testLabels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainImagesFlatten shape:  (784, 60000)\n",
      "trainLabelsFlatten shape:  (1, 60000)\n",
      "testImagesFlatten shape:  (784, 10000)\n",
      "testLabelsFlatten shape:  (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "trainImagesFlatten = trainImages.reshape(m_train, -1).T\n",
    "trainLabelsFlatten = trainLabels.reshape(1, trainLabels.shape[0])\n",
    "testImagesFlatten = testImages.reshape(m_test, -1).T\n",
    "testLabelsFlatten = testLabels.reshape(1, testLabels.shape[0])\n",
    "\n",
    "print (\"trainImagesFlatten shape: \", trainImagesFlatten.shape)\n",
    "print (\"trainLabelsFlatten shape: \", trainLabelsFlatten.shape)\n",
    "print (\"testImagesFlatten shape: \", testImagesFlatten.shape)\n",
    "print (\"testLabelsFlatten shape: \", testLabelsFlatten.shape)\n",
    "\n",
    "# Standardize the dataset \n",
    "trainImages = trainImagesFlatten / 255\n",
    "testImages = testImagesFlatten / 255\n",
    "trainLabels = trainLabelsFlatten\n",
    "testLabels = testLabelsFlatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n"
     ]
    }
   ],
   "source": [
    "def oneHotEncoding(v, numOfClasses):\n",
    "    \"\"\"\n",
    "    This function hot encode a vector of (1, n) dimension\n",
    "    \n",
    "    Argument:\n",
    "    v -- a numpy array to be one-hot encoded\n",
    "    numOfClasses -- an integer with the number of classes\n",
    "    Returns:\n",
    "    vOneHot -- one hot encoded vector\n",
    "    \"\"\"\n",
    "    vOneHot = np.eye(numOfClasses)[np.array(v).reshape(-1)]\n",
    "    return vOneHot.reshape(list(v.shape)+[numOfClasses])\n",
    "\n",
    "trainLabels = oneHotEncoding(trainLabels, 10)\n",
    "trainLabels = trainLabels.reshape(trainLabels.shape[2], trainLabels.shape[1])\n",
    "\n",
    "testLabels = oneHotEncoding(testLabels, 10)\n",
    "testLabels = testLabels.reshape(testLabels.shape[2], testLabels.shape[1])\n",
    "\n",
    "print(trainLabels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeWeightsAndBias(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of random values of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector - number of parameters\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    np.random.seed(None)\n",
    "    w = np.random.randn(dim, 10) * 0.01\n",
    "    b = 0 #float(np.squeeze(np.random.randn(1)))\n",
    "    \n",
    "    assert(w.shape == (dim, 10))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Compute the softmax of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- softmax(z)\n",
    "    \"\"\"\n",
    "    expToZ = np.exp(z - np.max(z))\n",
    "    return np.divide(expToZ, np.sum(expToZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "W, b = initializeWeightsAndBias(trainImages.shape[0])\n",
    "assert softmax(np.dot(W.T, trainImages) + b).shape == (W.T.shape[0], trainImages.shape[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(A, Y):\n",
    "    \"\"\"\n",
    "    This loss formula is used to compute loss of multiclass data\n",
    "    A -- Computed softmax of the linear transformation on wigth vector and train images vector plus the bias\n",
    "    of size (1, number of examples). \n",
    "    Y -- true \"label\" vector (containing 0, 1, 2 ... 10 ) of size (1, number of examples)\n",
    "    \n",
    "    Return:\n",
    "    Loss -- Cross entropy loss -- - sum of hadamard\n",
    "    \"\"\"\n",
    "    #loss = -np.divide(np.sum(np.multiply(Y, np.log(A, where=(A!=0)))), A.shape[1])\n",
    "    #loss = -np.divide(np.sum(np.multiply(Y, np.log(A))), A.shape[1])\n",
    "    #return loss\n",
    "    return np.divide(np.sum(np.multiply(Y, np.log(A)) + np.multiply(1-Y,  np.log(1-A))), -60000) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'W' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e987952e0c8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainImages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainLabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#W = np.zeros(shape=(784, 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'W' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "print(W.shape)\n",
    "print(trainImages.shape)\n",
    "print(trainLabels.shape)\n",
    "#W = np.zeros(shape=(784, 1))\n",
    "print(len(trainImages[trainImages != 0]))\n",
    "#A = softmax(np.dot(W.T, trainImages) + b)\n",
    "trainLabels.shape\n",
    "print(crossEntropy(A, trainLabels))\n",
    "print(A.dtype)\n",
    "Y = trainLabels\n",
    "cost = np.divide(np.sum(np.multiply(Y, np.log(A)) + np.multiply(1-Y,  np.log(1-A))), -60000) \n",
    "print(cost)\n",
    "#print(\"\\n\")\n",
    "#print(cross_entropy(A, trainLabels))\n",
    "\n",
    "#print(trainLabels.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardAndBackwardPropagation(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px,  1)\n",
    "    b -- bias, a scalar\n",
    "    X -- training images of size (num_px * num_px, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0, 1, 2 ... 10 ) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \"\"\"\n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    m = X.shape[1]\n",
    "    A = softmax(np.dot(w.T, X) + b)\n",
    "    cost = crossEntropy(A, Y)\n",
    "    \n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dz = A - Y\n",
    "    dw = (1/m)*dz.dot(X.T)\n",
    "    db = (1/m)*np.sum(dz)\n",
    "    dw = dw.T\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "    #assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px, number of examples)\n",
    "    Y -- true \"label\" vector of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1-4 lines of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = forwardAndBackwardPropagation(w, b, X, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0, 1, 2,3 ... 9 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0,1,..9) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    #Y_prediction = np.zeros((10,m))\n",
    "    #w = w.reshape(X.shape[0], 10)\n",
    "    A = softmax(np.dot(w.T, X) + b)\n",
    "\n",
    "    '''\n",
    "    for i in range(A.shape[1]):\n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        if A[0,i] <= .5:\n",
    "            Y_prediction[0, i] = 0\n",
    "        else:\n",
    "            Y_prediction[0, i] = 1\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction\n",
    "    '''\n",
    "    return np.argmax(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "       \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
    "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
    "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "   \n",
    "    w, b = initializeWeightsAndBias(X_train.shape[0])\n",
    "    \n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 13.307528\n",
      "Cost after iteration 0: 13.307528\n",
      "Cost after iteration 1: 13.314535\n",
      "Cost after iteration 2: 13.338658\n",
      "Cost after iteration 3: 13.380815\n",
      "Cost after iteration 4: 13.441968\n",
      "Cost after iteration 5: 13.523125\n",
      "Cost after iteration 6: 13.625347\n",
      "Cost after iteration 7: 13.749745\n",
      "Cost after iteration 8: 13.897485\n",
      "Cost after iteration 9: 14.069779\n",
      "train accuracy: -52135690.0 %\n",
      "test accuracy: -4785190.0 %\n"
     ]
    }
   ],
   "source": [
    "d = model(trainImages, trainLabels, testImages, testLabels, num_iterations = 10, learning_rate = 0.125, print_cost = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
